{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import urllib.parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multipages - San Francisco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty lists\n",
    "\n",
    "address = []\n",
    "bedrooms = []\n",
    "bathrooms = []\n",
    "area = []\n",
    "year_build = []\n",
    "parking = []\n",
    "prices = []\n",
    "\n",
    "# url_part_1 \n",
    "url_part_1 = 'https://www.trulia.com'\n",
    "\n",
    "for i in range(1,26):\n",
    "    \n",
    "    #website\n",
    "    website: 'https://www.trulia.com/CA/San_Francisco/'+ str(i) +'_p/'\n",
    "    \n",
    "    #request\n",
    "    response = requests.get('https://www.trulia.com/CA/San_Francisco/'+ str(i) +'_p/')\n",
    "\n",
    "    #soup object\n",
    "    soup= BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    #result\n",
    "    results = soup.find_all('li', {'class':'SearchResultsList__WideCell-b7y9ki-2'})\n",
    "    results_update=[]\n",
    "    for r in results:\n",
    "        if r.has_attr('data-testid'):\n",
    "            results_update.append(r)\n",
    "    \n",
    "    # relative url\n",
    "    relative_url = []\n",
    "\n",
    "    for item in results_update:\n",
    "        for link in item.find_all('div',{'data-testid':'property-card-details'}):\n",
    "            relative_url.append(link.find('a').get('href'))\n",
    "    \n",
    "    # create absolute url\n",
    "    \n",
    "    url_joined = []\n",
    "    \n",
    "    for link_2 in relative_url:\n",
    "        url_joined.append(urllib.parse.urljoin(url_part_1, link_2))\n",
    "    \n",
    "    #loop through all joined links\n",
    "    \n",
    "    for link in url_joined:\n",
    "        response = requests.get(link)\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "\n",
    "        #address\n",
    "        try:\n",
    "            address.append(soup.find('span',{'data-testid':'home-details-summary-headline'}).get_text())\n",
    "        except:\n",
    "            address.append('')\n",
    "\n",
    "        #bedrooms\n",
    "        try:\n",
    "            bedrooms.append(soup.find('li',{'data-testid':'bed'}).get_text())\n",
    "        except:\n",
    "            bedrooms.append('')\n",
    "\n",
    "        #bathrooms\n",
    "        try:\n",
    "            bathrooms.append(soup.find('li',{'data-testid':'bath'}).get_text())\n",
    "        except:\n",
    "            bathrooms.append('')\n",
    "\n",
    "        #area\n",
    "        try:\n",
    "            area.append(soup.find('li',{'data-testid':'floor'}).get_text())\n",
    "        except:\n",
    "            area.append('')\n",
    "\n",
    "        #year build\n",
    "        try:\n",
    "            year_build.append(soup.find('div',string=\"Year Built\").findNext('div').get_text())\n",
    "        except:\n",
    "            year_build.append('')\n",
    "\n",
    "        #parking\n",
    "        try:\n",
    "            parking.append(soup.find('div',string=\"Parking\").findNext('div').get_text())\n",
    "        except:\n",
    "            parking.append('')\n",
    "\n",
    "        #prices\n",
    "        try:\n",
    "            prices.append(soup.find('h3',{'data-testid':'on-market-price-details'}).get_text())\n",
    "        except:\n",
    "            prices.append('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_estate_final = {'Address':address, \n",
    "               'Bedrooms':bedrooms, \n",
    "               'Bathrooms':bathrooms,\n",
    "               'Area':area,\n",
    "               'Year Build':year_build,\n",
    "               'Parking':parking,\n",
    "               'Price':prices,\n",
    "            \n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_estate_final = pd.DataFrame(real_estate_final)\n",
    "real_estate_final['Location'] = 'San Francisco'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_estate_final.to_excel('real_estate_san_francisco.xlsx', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multipages - Washington"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty lists\n",
    "\n",
    "address = []\n",
    "bedrooms = []\n",
    "bathrooms = []\n",
    "area = []\n",
    "year_build = []\n",
    "parking = []\n",
    "prices = []\n",
    "\n",
    "# url_part_1 \n",
    "url_part_1 = 'https://www.trulia.com'\n",
    "\n",
    "for i in range(1,26):\n",
    "    \n",
    "    #website\n",
    "    website: 'https://www.trulia.com/DC/Washington/'+ str(i) +'_p/'\n",
    "    \n",
    "    #request\n",
    "    response = requests.get('https://www.trulia.com/DC/Washington/'+ str(i) +'_p/')\n",
    "\n",
    "    #soup object\n",
    "    soup= BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    #result\n",
    "    results = soup.find_all('li', {'class':'SearchResultsList__WideCell-b7y9ki-2'})\n",
    "    results_update=[]\n",
    "    for r in results:\n",
    "        if r.has_attr('data-testid'):\n",
    "            results_update.append(r)\n",
    "    \n",
    "    # relative url\n",
    "    relative_url = []\n",
    "\n",
    "    for item in results_update:\n",
    "        for link in item.find_all('div',{'data-testid':'property-card-details'}):\n",
    "            relative_url.append(link.find('a').get('href'))\n",
    "    \n",
    "    # create absolute url\n",
    "    \n",
    "    url_joined = []\n",
    "    \n",
    "    for link_2 in relative_url:\n",
    "        url_joined.append(urllib.parse.urljoin(url_part_1, link_2))\n",
    "    \n",
    "    #loop through all joined links\n",
    "    \n",
    "    for link in url_joined:\n",
    "        response = requests.get(link)\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "\n",
    "        #address\n",
    "        try:\n",
    "            address.append(soup.find('span',{'data-testid':'home-details-summary-headline'}).get_text())\n",
    "        except:\n",
    "            address.append('')\n",
    "\n",
    "        #bedrooms\n",
    "        try:\n",
    "            bedrooms.append(soup.find('li',{'data-testid':'bed'}).get_text())\n",
    "        except:\n",
    "            bedrooms.append('')\n",
    "\n",
    "        #bathrooms\n",
    "        try:\n",
    "            bathrooms.append(soup.find('li',{'data-testid':'bath'}).get_text())\n",
    "        except:\n",
    "            bathrooms.append('')\n",
    "\n",
    "        #area\n",
    "        try:\n",
    "            area.append(soup.find('li',{'data-testid':'floor'}).get_text())\n",
    "        except:\n",
    "            area.append('')\n",
    "\n",
    "        #year build\n",
    "        try:\n",
    "            year_build.append(soup.find('div',string=\"Year Built\").findNext('div').get_text())\n",
    "        except:\n",
    "            year_build.append('')\n",
    "\n",
    "        #parking\n",
    "        try:\n",
    "            parking.append(soup.find('div',string=\"Parking\").findNext('div').get_text())\n",
    "        except:\n",
    "            parking.append('')\n",
    "\n",
    "        #prices\n",
    "        try:\n",
    "            prices.append(soup.find('h3',{'data-testid':'on-market-price-details'}).get_text())\n",
    "        except:\n",
    "            prices.append('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_estate_final = {'Address':address, \n",
    "               'Bedrooms':bedrooms, \n",
    "               'Bathrooms':bathrooms,\n",
    "               'Area':area,\n",
    "               'Year Build':year_build,\n",
    "               'Parking':parking,\n",
    "               'Price':prices,\n",
    "                     \n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_estate_final = pd.DataFrame(real_estate_final)\n",
    "real_estate_final['Location'] = 'Washington'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_estate_final.to_excel('real_estate_washington.xlsx', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multipages - New York"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty lists\n",
    "\n",
    "address = []\n",
    "bedrooms = []\n",
    "bathrooms = []\n",
    "area = []\n",
    "year_build = []\n",
    "parking = []\n",
    "prices = []\n",
    "\n",
    "# url_part_1 \n",
    "url_part_1 = 'https://www.trulia.com'\n",
    "\n",
    "for i in range(1,26):\n",
    "    \n",
    "    #website\n",
    "    website: 'https://www.trulia.com/NY/New_York/'+ str(i) +'_p/'\n",
    "    \n",
    "    #request\n",
    "    response = requests.get('https://www.trulia.com/NY/New_York/'+ str(i) +'_p/')\n",
    "\n",
    "    #soup object\n",
    "    soup= BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    #result\n",
    "    results = soup.find_all('li', {'class':'SearchResultsList__WideCell-b7y9ki-2'})\n",
    "    results_update=[]\n",
    "    for r in results:\n",
    "        if r.has_attr('data-testid'):\n",
    "            results_update.append(r)\n",
    "    \n",
    "    # relative url\n",
    "    relative_url = []\n",
    "\n",
    "    for item in results_update:\n",
    "        for link in item.find_all('div',{'data-testid':'property-card-details'}):\n",
    "            relative_url.append(link.find('a').get('href'))\n",
    "    \n",
    "    # create absolute url\n",
    "    \n",
    "    url_joined = []\n",
    "    \n",
    "    for link_2 in relative_url:\n",
    "        url_joined.append(urllib.parse.urljoin(url_part_1, link_2))\n",
    "    \n",
    "    #loop through all joined links\n",
    "    \n",
    "    for link in url_joined:\n",
    "        response = requests.get(link)\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "\n",
    "        #address\n",
    "        try:\n",
    "            address.append(soup.find('span',{'data-testid':'home-details-summary-headline'}).get_text())\n",
    "        except:\n",
    "            address.append('')\n",
    "\n",
    "        #bedrooms\n",
    "        try:\n",
    "            bedrooms.append(soup.find('li',{'data-testid':'bed'}).get_text())\n",
    "        except:\n",
    "            bedrooms.append('')\n",
    "\n",
    "        #bathrooms\n",
    "        try:\n",
    "            bathrooms.append(soup.find('li',{'data-testid':'bath'}).get_text())\n",
    "        except:\n",
    "            bathrooms.append('')\n",
    "\n",
    "        #area\n",
    "        try:\n",
    "            area.append(soup.find('li',{'data-testid':'floor'}).get_text())\n",
    "        except:\n",
    "            area.append('')\n",
    "\n",
    "        #year build\n",
    "        try:\n",
    "            year_build.append(soup.find('div',string=\"Year Built\").findNext('div').get_text())\n",
    "        except:\n",
    "            year_build.append('')\n",
    "\n",
    "        #parking\n",
    "        try:\n",
    "            parking.append(soup.find('div',string=\"Parking\").findNext('div').get_text())\n",
    "        except:\n",
    "            parking.append('')\n",
    "\n",
    "        #prices\n",
    "        try:\n",
    "            prices.append(soup.find('h3',{'data-testid':'on-market-price-details'}).get_text())\n",
    "        except:\n",
    "            prices.append('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_estate_final = {'Address':address, \n",
    "               'Bedrooms':bedrooms, \n",
    "               'Bathrooms':bathrooms,\n",
    "               'Area':area,\n",
    "               'Year Build':year_build,\n",
    "               'Parking':parking,\n",
    "               'Price':prices,\n",
    "                     \n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_estate_final = pd.DataFrame(real_estate_final)\n",
    "real_estate_final['Location'] = 'New York'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_estate_final.to_excel('real_estate_new_york.xlsx', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multipages - Miami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty lists\n",
    "\n",
    "address = []\n",
    "bedrooms = []\n",
    "bathrooms = []\n",
    "area = []\n",
    "year_build = []\n",
    "parking = []\n",
    "prices = []\n",
    "\n",
    "# url_part_1 \n",
    "url_part_1 = 'https://www.trulia.com'\n",
    "\n",
    "for i in range(1,26):\n",
    "    \n",
    "    #website\n",
    "    website: 'https://www.trulia.com/FL/Miami/'+ str(i) +'_p/'\n",
    "    \n",
    "    #request\n",
    "    response = requests.get('https://www.trulia.com/FL/Miami/'+ str(i) +'_p/')\n",
    "\n",
    "    #soup object\n",
    "    soup= BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    #result\n",
    "    results = soup.find_all('li', {'class':'SearchResultsList__WideCell-b7y9ki-2'})\n",
    "    results_update=[]\n",
    "    for r in results:\n",
    "        if r.has_attr('data-testid'):\n",
    "            results_update.append(r)\n",
    "    \n",
    "    # relative url\n",
    "    relative_url = []\n",
    "\n",
    "    for item in results_update:\n",
    "        for link in item.find_all('div',{'data-testid':'property-card-details'}):\n",
    "            relative_url.append(link.find('a').get('href'))\n",
    "    \n",
    "    # create absolute url\n",
    "    \n",
    "    url_joined = []\n",
    "    \n",
    "    for link_2 in relative_url:\n",
    "        url_joined.append(urllib.parse.urljoin(url_part_1, link_2))\n",
    "    \n",
    "    #loop through all joined links\n",
    "    \n",
    "    for link in url_joined:\n",
    "        response = requests.get(link)\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "\n",
    "        #address\n",
    "        try:\n",
    "            address.append(soup.find('span',{'data-testid':'home-details-summary-headline'}).get_text())\n",
    "        except:\n",
    "            address.append('')\n",
    "\n",
    "        #bedrooms\n",
    "        try:\n",
    "            bedrooms.append(soup.find('li',{'data-testid':'bed'}).get_text())\n",
    "        except:\n",
    "            bedrooms.append('')\n",
    "\n",
    "        #bathrooms\n",
    "        try:\n",
    "            bathrooms.append(soup.find('li',{'data-testid':'bath'}).get_text())\n",
    "        except:\n",
    "            bathrooms.append('')\n",
    "\n",
    "        #area\n",
    "        try:\n",
    "            area.append(soup.find('li',{'data-testid':'floor'}).get_text())\n",
    "        except:\n",
    "            area.append('')\n",
    "\n",
    "        #year build\n",
    "        try:\n",
    "            year_build.append(soup.find('div',string=\"Year Built\").findNext('div').get_text())\n",
    "        except:\n",
    "            year_build.append('')\n",
    "\n",
    "        #parking\n",
    "        try:\n",
    "            parking.append(soup.find('div',string=\"Parking\").findNext('div').get_text())\n",
    "        except:\n",
    "            parking.append('')\n",
    "\n",
    "        #prices\n",
    "        try:\n",
    "            prices.append(soup.find('h3',{'data-testid':'on-market-price-details'}).get_text())\n",
    "        except:\n",
    "            prices.append('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_estate_final = {'Address':address, \n",
    "               'Bedrooms':bedrooms, \n",
    "               'Bathrooms':bathrooms,\n",
    "               'Area':area,\n",
    "               'Year Build':year_build,\n",
    "               'Parking':parking,\n",
    "               'Price':prices,\n",
    "                     \n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_estate_final = pd.DataFrame(real_estate_final)\n",
    "real_estate_final['Location'] = 'Miami'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_estate_final.to_excel('real_estate_miami.xlsx', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
